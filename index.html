<!DOCTYPE html>
<html lang="it">
<head>
  <meta charset="UTF-8">
  <title>Demo YOLO-Pose nel Browser</title>
  <!-- ONNX Runtime Web per caricare il modello ONNX -->
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
  <style>
    body { margin: 0; overflow: hidden; }
    video, canvas { position: absolute; top: 0; left: 0; }
  </style>
</head>
<body>
  <!-- Video dalla webcam (nascosto) -->
  <video id="video" width="640" height="480" autoplay muted style="display:none;"></video>
  <!-- Canvas per disegnare video e pose -->
  <canvas id="output" width="640" height="480"></canvas>

  <script>
    async function init() {
      const video = document.getElementById('video');
      const canvas = document.getElementById('output');
      const ctx = canvas.getContext('2d');

      // 1) Attiva la webcam
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ video: true });
        video.srcObject = stream;
        await video.play();
      } catch (e) {
        alert('Impossibile accedere alla webcam: ' + e.message);
        return;
      }

      // 2) Carica il modello YOLO-Pose (ONNX). Scarica yolov8n-pose.onnx da Ultralytics
      const session = await ort.InferenceSession.create('yolov8n-pose.onnx');
      const inputSize = 640;  // YOLOv8n-pose richiede esattamente 640x640
      
      // Debug: controlla i metadati del modello
      console.log('Model input names:', session.inputNames);
      console.log('Model output names:', session.outputNames);
      console.log('Session inputs:', session.inputs);
      console.log('Session outputs:', session.outputs);

      // 3) Loop di rilevamento
      async function detectFrame() {
        // Controlla se il video è pronto
        if (video.readyState !== video.HAVE_ENOUGH_DATA) {
          requestAnimationFrame(detectFrame);
          return;
        }
        
        // Pulisci il canvas prima di disegnare
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        
        // Disegna l'immagine corrente della webcam nel canvas
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

        // Preprocess: ottieni dati pixel e normalizza
        const tensorData = preprocess(video, inputSize, canvas.width, canvas.height);
        const tensor = new ort.Tensor('float32', tensorData, [1, 3, inputSize, inputSize]);
        
        // Debug: controlla i dati di input (solo una volta)
        if (!window.debugLogged) {
          console.log('Input tensor OK - data processed correctly');
          window.debugLogged = true;
        }

        // Inference
        const feeds = { 'images': tensor };
        const results = await session.run(feeds);
        
        // Debug: controlla l'output del modello (solo una volta)
        if (!window.outputLogged) {
          console.log('Model output keys:', Object.keys(results));
          Object.keys(results).forEach(key => {
            const output = results[key];
            console.log(`Output '${key}': shape=${output.dims}, data length=${output.data.length}`);
          });
          window.outputLogged = true;
        }

        // Postprocess: estrae keypoints e scala alle dimensioni video
        const keypoints = postprocess(results, canvas.width, canvas.height, inputSize);

        // Disegna la posa
        drawPose(ctx, keypoints);
        
        // Debug: disegna solo i keypoints, non bbox
        // if (keypoints.length > 0) {
        //   drawDebugInfo(ctx, results, canvas.width, canvas.height, inputSize);
        // }

        // Prossimo frame (con controllo per evitare ricorsione infinita)
        setTimeout(() => {
          if (!window.stopDetection) {
            detectFrame();
          }
        }, 100); // 10 FPS invece di 60 FPS
      }
      detectFrame();
    }

    function preprocess(video, size, origWidth, origHeight) {
      // Semplice ridimensionamento senza padding
      const canvas = document.createElement('canvas');
      canvas.width = size; 
      canvas.height = size;
      const ctx2 = canvas.getContext('2d');
      
      // Disegna l'immagine ridimensionata direttamente
      ctx2.drawImage(video, 0, 0, size, size);
      
      const resized = ctx2.getImageData(0, 0, size, size).data;
      const tensor = new Float32Array(3 * size * size);
      
              // Debug: controlla i dati dell'immagine (solo una volta)
        if (!window.imageLogged) {
          console.log('Image data OK - video loaded correctly');
          window.imageLogged = true;
        }
      
      // Ottimizzazione: processa in blocchi per migliori performance
      const sizeSquared = size * size;
      for (let i = 0; i < sizeSquared; i++) {
        const pixelIndex = i * 4;
        const tensorIndex = i;
        // Normalizzazione RGB [0,1] -> CHW format
        tensor[tensorIndex] = resized[pixelIndex] / 255.0;                    // R
        tensor[sizeSquared + tensorIndex] = resized[pixelIndex + 1] / 255.0;  // G  
        tensor[2 * sizeSquared + tensorIndex] = resized[pixelIndex + 2] / 255.0; // B
      }
      return tensor;
    }

    function postprocess(results, outW, outH, inputSize) {
      // Estrae l'array di keypoints dal risultato ONNX
      // Trova il primo output disponibile
      const outputNames = Object.keys(results);
      
      if (outputNames.length === 0) {
        console.error('No outputs found from model');
        return [];
      }
      
      // Prova prima 'output0', poi 'output', poi il primo disponibile
      let outputData;
      if (results['output0']) {
        outputData = results['output0'].data;
      } else if (results['output']) {
        outputData = results['output'].data;
      } else {
        outputData = results[outputNames[0]].data;
      }
      
      // Analizziamo l'output del modello per capire il formato reale
      console.log('=== ANALISI FORMATO OUTPUT ===');
      console.log('Output data length:', outputData.length);
      
      // Proviamo diverse interpretazioni del formato
      const possibleFormats = [
        { name: '56x8400', numAttrs: 56, numDets: outputData.length / 56 },
        { name: '8400x56', numAttrs: outputData.length / 8400, numDets: 8400 },
        { name: '17x3x8400', numAttrs: 51, numDets: outputData.length / 51 },
        { name: '56x56x150', numAttrs: 56, numDets: outputData.length / 56 }
      ];
      
      console.log('Possibili formati:');
      possibleFormats.forEach(fmt => {
        console.log(`${fmt.name}: ${fmt.numAttrs} attributi, ${fmt.numDets} detections`);
      });
      
      // Per ora usiamo il formato originale ma con debug
      const numAttributes = 56;
      const numDetections = outputData.length / numAttributes;
      
      console.log('Output data length:', outputData.length);
      console.log('Number of detections:', numDetections);
      console.log('Sample values:', outputData.slice(0, 10));
      
      // Analisi approfondita dell'output
      console.log('=== ANALISI OUTPUT ===');
      console.log('Primi 56 valori (prima detection):', outputData.slice(0, 56));
      console.log('Valori 5-9 (bbox + conf):', outputData.slice(5, 10));
      console.log('Valori 10-15 (primi keypoints):', outputData.slice(10, 15));
      
      // Trova tutte le detections con confidence > 0.1
      let validDetections = [];
      for (let det = 0; det < Math.min(numDetections, 10); det++) {
        const confIndex = det * numAttributes + 4;
        const conf = outputData[confIndex];
        if (conf > 0.1) {
          const bboxX = outputData[det * numAttributes + 0];
          const bboxY = outputData[det * numAttributes + 1];
          validDetections.push({det, conf, bboxX, bboxY});
        }
      }
      console.log('Detections valide:', validDetections);
      
      if (numDetections < 1) {
        console.log('No detections found');
        return [];
      }
      
      // Il modello sembra restituire solo keypoints, non bbox
      // Proviamo a interpretare l'output come 17 keypoints per detection
      const keypointsPerDetection = 17;
      const valuesPerKeypoint = 3; // x, y, conf
      const totalValuesPerDetection = keypointsPerDetection * valuesPerKeypoint; // 51
      const numDetectionsKeypoints = outputData.length / totalValuesPerDetection;
      
      console.log(`Interpretazione keypoints-only: ${numDetectionsKeypoints} detections, ${keypointsPerDetection} keypoints per detection`);
      
      // Trova la detection con confidence media più alta
      let bestDetection = -1;
      let bestConfidence = 0;
      
      for (let det = 0; det < Math.min(numDetectionsKeypoints, 10); det++) {
        let totalConf = 0;
        let validKeypoints = 0;
        
        for (let kpt = 0; kpt < keypointsPerDetection; kpt++) {
          const confIndex = det * totalValuesPerDetection + kpt * 3 + 2; // conf è il 3° valore
          const conf = outputData[confIndex];
          if (conf > 0.1) {
            totalConf += conf;
            validKeypoints++;
          }
        }
        
        const avgConf = validKeypoints > 0 ? totalConf / validKeypoints : 0;
        if (avgConf > bestConfidence) {
          bestConfidence = avgConf;
          bestDetection = det;
          console.log(`Detection ${det}: avg conf ${avgConf.toFixed(3)}, valid keypoints ${validKeypoints}`);
        }
      }
      
      if (bestConfidence > 0.01) { // Threshold più basso
        console.log('Detection found! Confidence:', bestConfidence.toFixed(3));
        console.log('Best detection index:', bestDetection);
        
        // Estrai i keypoints dalla migliore detection (formato keypoints-only)
        console.log('Keypoints from best detection:');
        for (let i = 0; i < 17; i++) {
          const xIndex = bestDetection * totalValuesPerDetection + i * 3; // x è il 1° valore
          const yIndex = bestDetection * totalValuesPerDetection + i * 3 + 1; // y è il 2° valore
          const confIndex = bestDetection * totalValuesPerDetection + i * 3 + 2; // conf è il 3° valore
          
          const x = outputData[xIndex];
          const y = outputData[yIndex];
          const conf = outputData[confIndex];
          
          // Normalizza anche i keypoints
          const normalizedX = x / 640.0;
          const normalizedY = y / 640.0;
          const normalizedConf = conf / 1000.0;
          
          console.log(`Keypoint ${i}: raw(${x.toFixed(1)}, ${y.toFixed(1)}, ${conf.toFixed(1)}) -> norm(${normalizedX.toFixed(3)}, ${normalizedY.toFixed(3)}, ${normalizedConf.toFixed(3)})`);
        }
      }
      
      const keypoints = [];
      
      if (bestConfidence > 0.01 && bestDetection >= 0) { // Threshold più basso
        // Estrai i 17 keypoints dalla migliore detection (formato keypoints-only)
        for (let i = 0; i < 17; i++) {
          const xIndex = bestDetection * totalValuesPerDetection + i * 3; // x è il 1° valore
          const yIndex = bestDetection * totalValuesPerDetection + i * 3 + 1; // y è il 2° valore
          const confIndex = bestDetection * totalValuesPerDetection + i * 3 + 2; // conf è il 3° valore
          
          // Le coordinate sono assolute, normalizziamole
          const modelX = outputData[xIndex];
          const modelY = outputData[yIndex];
          const kptConf = outputData[confIndex];
          
          // Normalizza le coordinate
          const normalizedX = modelX / 640.0;
          const normalizedY = modelY / 640.0;
          const normalizedConf = kptConf / 1000.0;
          
          // Scaling: da coordinate normalizzate (0-1) a coordinate output
          const x = normalizedX * outW;
          const y = normalizedY * outH;
          
          if (normalizedConf > 0.1) { // Threshold più basso
            keypoints.push([x, y]);
          } else {
            keypoints.push(null);
          }
        }
      }
      
      return keypoints;
    }

    function drawPose(ctx, keypoints) {
      ctx.lineWidth = 2;
      ctx.strokeStyle = 'red';
      ctx.fillStyle = 'red';
      // Disegna punti
      keypoints.forEach(pt => {
        if (pt) {
          ctx.beginPath(); ctx.arc(pt[0], pt[1], 5, 0, 2*Math.PI);
          ctx.fill();
        }
      });
      // Definisci connessioni scheletriche COCO 17 keypoints:
      // 0:nose, 1:left_eye, 2:right_eye, 3:left_ear, 4:right_ear
      // 5:left_shoulder, 6:right_shoulder, 7:left_elbow, 8:right_elbow
      // 9:left_wrist, 10:right_wrist, 11:left_hip, 12:right_hip
      // 13:left_knee, 14:right_knee, 15:left_ankle, 16:right_ankle
      const skeleton = [
        [0,1], [0,2],               // naso agli occhi
        [1,3], [2,4],               // occhi alle orecchie
        [5,6],                      // spalle
        [5,7], [7,9],               // braccio sinistro
        [6,8], [8,10],              // braccio destro
        [5,11], [6,12],             // spalle ai fianchi
        [11,12],                    // fianchi
        [11,13], [13,15],           // gamba sinistra
        [12,14], [14,16]            // gamba destra
      ];
      skeleton.forEach(([i,j]) => {
        if (keypoints[i] && keypoints[j]) {
          ctx.beginPath();
          ctx.moveTo(keypoints[i][0], keypoints[i][1]);
          ctx.lineTo(keypoints[j][0], keypoints[j][1]);
          ctx.stroke();
        }
      });
    }

    function drawDebugInfo(ctx, results, outW, outH, inputSize) {
      // Trova la migliore detection e disegna il bounding box
      const outputNames = Object.keys(results);
      if (outputNames.length === 0) return;
      
      let outputData;
      if (results['output0']) {
        outputData = results['output0'].data;
      } else if (results['output']) {
        outputData = results['output'].data;
      } else {
        outputData = results[outputNames[0]].data;
      }
      
      const numAttributes = 56;
      const numDetections = outputData.length / numAttributes;
      
      let bestDetection = -1;
      let bestConfidence = 0;
      
      for (let det = 0; det < numDetections; det++) {
        const confIndex = det * numAttributes + 4;
        const conf = outputData[confIndex];
        if (conf > bestConfidence) {
          bestConfidence = conf;
          bestDetection = det;
        }
      }
      
      if (bestConfidence > 0.1 && bestDetection >= 0) {
        // Calcola coordinate bbox corrette
        // Le coordinate sono assolute, normalizziamole
        const modelBboxX = outputData[bestDetection * numAttributes + 0];
        const modelBboxY = outputData[bestDetection * numAttributes + 1];
        const modelBboxW = outputData[bestDetection * numAttributes + 2];
        const modelBboxH = outputData[bestDetection * numAttributes + 3];
        
        // Normalizza le coordinate
        const normalizedBboxX = modelBboxX / 640.0;
        const normalizedBboxY = modelBboxY / 640.0;
        const normalizedBboxW = modelBboxW / 640.0;
        const normalizedBboxH = modelBboxH / 640.0;
        
        // Scaling: da coordinate normalizzate (0-1) a coordinate output
        const bboxX = normalizedBboxX * outW;
        const bboxY = normalizedBboxY * outH;
        const bboxW = normalizedBboxW * outW;
        const bboxH = normalizedBboxH * outH;
        
        // Disegna bounding box
        ctx.strokeStyle = 'lime';
        ctx.lineWidth = 2;
        ctx.beginPath();
        ctx.rect(bboxX - bboxW/2, bboxY - bboxH/2, bboxW, bboxH);
        ctx.stroke();
        
        // Mostra confidence
        ctx.fillStyle = 'lime';
        ctx.font = '16px Arial';
        ctx.fillText(`Conf: ${bestConfidence.toFixed(2)}`, bboxX - bboxW/2, bboxY - bboxH/2 - 5);
        
        // Debug: mostra coordinate
        console.log(`Detection: center(${bboxX.toFixed(1)}, ${bboxY.toFixed(1)}), size(${bboxW.toFixed(1)}x${bboxH.toFixed(1)})`);
      }
    }

    // Avvia l'app
    init();
  </script>
</body>
</html>

<!--
ISTRUZIONI:
1. Scarica il modello ONNX YOLO-Pose (es. yolov8n-pose.onnx) da https://github.com/ultralytics/ultralytics
2. Metti il file `yolov8n-pose.onnx` nella stessa cartella di `index.html`
3. Avvia un semplice server HTTP: `python3 -m http.server 8000`
4. Apri nel browser: http://localhost:8000/index.html
-->
